import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import firebase_admin
from firebase_admin import credentials, firestore, messaging
import os
import hashlib
import re

# --- ì„¤ì •ê°’ ---
LH_BASE_URL = "https://www.lh.or.kr"
BOARD_MID = "a10601020000"
BOARD_BID = "0034"

def init_firebase():
    """Firebase ì´ˆê¸°í™”"""
    if not firebase_admin._apps:
        # í˜„ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— serviceAccountKey.jsonì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        cred_path = os.path.join(os.path.dirname(__file__), 'serviceAccountKey.json')
        if not os.path.exists(cred_path):
            raise FileNotFoundError(f"í‚¤ íŒŒì¼ ì—†ìŒ: {cred_path}")
        
        cred = credentials.Certificate(cred_path)
        firebase_admin.initialize_app(cred)
    return firestore.client()

def send_fcm_notification(title, link, source='LH'):
    """FCM ì•Œë¦¼ ë°œì†¡ í•¨ìˆ˜ - ì‹ ë¢°ì„± ê°œì„ """
    try:
        # ì†ŒìŠ¤ë³„ ì•Œë¦¼ ì œëª© ì„¤ì •
        source_names = {
            'LH': 'LH ê³µëª¨ ì•Œë¦¼',
            'KAMS': 'ì˜ˆìˆ ê²½ì˜ì§€ì›ì„¼í„° ì•Œë¦¼',
            'Seoul': 'ì„œìš¸íŠ¹ë³„ì‹œ ì•Œë¦¼',
            'SeoulPublicArt': 'ì„œìš¸ ê³µê³µë¯¸ìˆ  ê³µëª¨ ì•Œë¦¼'
        }
        source_name = source_names.get(source, 'ê³µëª¨ ì•Œë¦¼')
        
        # 'lh_notice'ë¼ëŠ” ì£¼ì œ(Topic)ë¥¼ êµ¬ë…í•œ ì•±ë“¤ì—ê²Œ ì•Œë¦¼ì„ ì©ë‹ˆë‹¤.
        message = messaging.Message(
            notification=messaging.Notification(
                title=f"[{source_name}]",
                body=title,
            ),
            data={
                'link': link, # ì•±ì—ì„œ í´ë¦­ ì‹œ ì´ë™í•  ë§í¬
                'source': source, # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                'click_action': 'FLUTTER_NOTIFICATION_CLICK'
            },
            topic='lh_notice',
            # ì•Œë¦¼ ìš°ì„ ìˆœìœ„ ì„¤ì • (ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ì¦‰ì‹œ ì „ë‹¬)
            android=messaging.AndroidConfig(
                priority='high',
                notification=messaging.AndroidNotification(
                    priority='high',
                    sound='default',
                    channel_id='lh_notice_channel'
                )
            ),
        )
        response = messaging.send(message)
        print(f"  ğŸ“¢ [ì•Œë¦¼ ë°œì†¡ ì„±ê³µ] Message ID: {response} | Source: {source}")
        return True
    except Exception as e:
        print(f"  âš ï¸ [ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨] {e}")
        import traceback
        traceback.print_exc()
        return False

def check_and_save(db, data, source='LH'):
    """ì €ì¥ ë° ì•Œë¦¼ íŠ¸ë¦¬ê±° - source í•„ë“œ ì¶”ê°€"""
    link = data.get('link', '').strip()
    if not link or link == '#': return False
    
    doc_id = hashlib.md5(link.encode('utf-8')).hexdigest()
    
    try:
        notices_ref = db.collection('notices')
        doc_ref = notices_ref.document(doc_id)
        
        # 1. ì´ë¯¸ ì €ì¥ëœ ê¸€ì¸ì§€ í™•ì¸
        if doc_ref.get().exists:
            # print(f"  [ì¤‘ë³µ] {data['title']}") # ë„ˆë¬´ ì‹œë„ëŸ¬ìš°ë©´ ì£¼ì„ ì²˜ë¦¬
            return False 
        
        # 2. ì‹ ê·œ ì €ì¥ (source í•„ë“œ ì¶”ê°€)
        doc_ref.set({
            'number': data.get('number', ''),
            'title': data.get('title', ''),
            'date': data.get('date', ''),
            'link': link,
            'source': source,  # ì†ŒìŠ¤ í•„ë“œ ì¶”ê°€
            'created_at': firestore.SERVER_TIMESTAMP
        })
        
        # 3. [ì¤‘ìš”] ì €ì¥ ì„±ê³µ ì‹œ ì•Œë¦¼ ë°œì†¡ í•¨ìˆ˜ í˜¸ì¶œ!
        print(f"  ğŸ’¾ [ì‹ ê·œ ì €ì¥ ì™„ë£Œ] {data['title']} | Source: {source}")
        notification_sent = send_fcm_notification(data['title'], link, source)
        
        if not notification_sent:
            print(f"  âš ï¸ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨í–ˆì§€ë§Œ ë°ì´í„°ëŠ” ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
    except Exception as e:
        print(f"  DB ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        return False

def crawl_lh_notice():
    list_url = f"{LH_BASE_URL}/board.es?mid={BOARD_MID}&bid={BOARD_BID}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print(f"--- í¬ë¡¤ë§ ì‹œì‘: {list_url} ---")
    
    try:
        response = requests.get(list_url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        
        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select('table tbody tr')
        
        if not rows:
            print("âŒ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        results = []
        for row in rows:
            # ... (ë°ì´í„° ì¶”ì¶œ ë¡œì§ì€ ì•„ê¹Œ ê²€ì¦ëœ ì½”ë“œì™€ ë™ì¼) ...
            cells = row.find_all(['td', 'th'])
            if len(cells) < 3: continue
            
            number = cells[0].get_text(strip=True)
            link_tag = row.find('a')
            if not link_tag: continue

            title = link_tag.get_text(strip=True).replace('ìƒˆê¸€', '').strip()
            
            link_path = link_tag.get('href', '').strip()
            onclick = link_tag.get('onclick', '')
            final_link = ""

            if link_path and not link_path.startswith('java') and link_path != '#' and link_path != '#none':
                 final_link = urljoin(LH_BASE_URL, link_path)
            elif onclick:
                # onclickì—ì„œ ì‹¤ì œ ë§í¬ ê²½ë¡œ ì¶”ì¶œ
                # ì˜ˆ: goView3('729895','/board.es?mid=a10601020000&bid=0034&act=view&list_no=729895&tag=&nPage=1');
                match = re.search(r"['\"](/board\.es\?[^'\"]+)['\"]", onclick)
                if match:
                    # onclickì—ì„œ ì „ì²´ ê²½ë¡œ ì¶”ì¶œ
                    path = match.group(1)
                    final_link = urljoin(LH_BASE_URL, path)
                else:
                    # í´ë°±: list_noë§Œ ì¶”ì¶œí•´ì„œ ë§í¬ ìƒì„±
                    match = re.search(r"['\"]?(\d{4,})['\"]?", onclick) 
                    if match:
                        list_no = match.group(1)
                        final_link = f"{LH_BASE_URL}/board.es?mid={BOARD_MID}&bid={BOARD_BID}&act=view&list_no={list_no}&tag=&nPage=1"
            
            if not final_link: continue

            # ë‚ ì§œ ì¶”ì¶œ
            date_text = cells[-2].get_text(strip=True)
            if not re.search(r'\d{4}[.-]\d{2}[.-]\d{2}', date_text):
                for cell in cells:
                    if re.search(r'\d{4}[.-]\d{2}[.-]\d{2}', cell.get_text(strip=True)):
                        date_text = cell.get_text(strip=True)
                        break

            # ëª¨ë“  ê²Œì‹œê¸€ ì €ì¥ (í‚¤ì›Œë“œ í•„í„°ë§ ì œê±°)
            results.append({
                'number': number,
                'title': title,
                'date': date_text,
                'link': final_link
            })

        # DB ì €ì¥ ë° ì•Œë¦¼ ì‹œë„
        if results:
            print(f"ì´ {len(results)}ê±´ì˜ ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            db = init_firebase()
            new_count = 0
            for item in results:
                if check_and_save(db, item, source='LH'):
                    new_count += 1
            print(f"\n=== LH ì‹¤í–‰ ì™„ë£Œ: {new_count}ê±´ ì‹ ê·œ ì €ì¥ ë° ì•Œë¦¼ ì „ì†¡ ===")
        else:
            print("\nLH ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")

def crawl_kams_notice():
    """KAMS ì˜ˆìˆ ê²½ì˜ì§€ì›ì„¼í„° í¬ë¡¤ë§"""
    list_url = "https://gokams.or.kr/01_news/event_list.aspx"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print(f"--- KAMS í¬ë¡¤ë§ ì‹œì‘: {list_url} ---")
    
    try:
        response = requests.get(list_url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # KAMS ì‚¬ì´íŠ¸ êµ¬ì¡°: table tr í˜•íƒœ
        rows = soup.select('table tbody tr, table tr')
        
        if not rows:
            print("âŒ KAMS ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        results = []
        base_url = "https://gokams.or.kr/01_news/"
        
        for row in rows:
            try:
                cells = row.find_all('td')
                if len(cells) < 4:  # ìµœì†Œ 4ê°œ ì…€ í•„ìš”
                    continue
                
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                link_tag = row.find('a')
                if not link_tag:
                    continue
                
                title = link_tag.get_text(strip=True)
                # "new" ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì œê±°
                title = re.sub(r'\s*\[new\]\s*', '', title, flags=re.IGNORECASE)
                if not title:
                    continue
                
                # ë§í¬ ì¶”ì¶œ
                href = link_tag.get('href', '')
                if not href or href == '#':
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if href.startswith('/'):
                    final_link = urljoin('https://gokams.or.kr', href)
                elif href.startswith('http'):
                    final_link = href
                else:
                    final_link = urljoin(base_url, href)
                
                # ë‚ ì§œ ì¶”ì¶œ (ì¼ë°˜ì ìœ¼ë¡œ 4ë²ˆì§¸ ë˜ëŠ” 5ë²ˆì§¸ ì…€)
                date_text = ''
                for cell in cells:
                    cell_text = cell.get_text(strip=True)
                    # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD ë˜ëŠ” YYYY-MM-DD ~ MM-DD)
                    date_match = re.search(r'(\d{4}[.-]\d{2}[.-]\d{2})', cell_text)
                    if date_match:
                        date_text = cell_text
                        break
                
                if not date_text:
                    date_text = 'ë‚ ì§œ ì—†ìŒ'
                
                # ë²ˆí˜¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì…€)
                number = cells[0].get_text(strip=True) if cells else ''
                
                results.append({
                    'number': number,
                    'title': title,
                    'date': date_text,
                    'link': final_link
                })
            except Exception as e:
                print(f"  âš ï¸ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        # DB ì €ì¥ ë° ì•Œë¦¼ ì‹œë„
        if results:
            print(f"ì´ {len(results)}ê±´ì˜ KAMS ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            db = init_firebase()
            new_count = 0
            for item in results:
                if check_and_save(db, item, source='KAMS'):
                    new_count += 1
            print(f"\n=== KAMS ì‹¤í–‰ ì™„ë£Œ: {new_count}ê±´ ì‹ ê·œ ì €ì¥ ë° ì•Œë¦¼ ì „ì†¡ ===")
        else:
            print("\nKAMS ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"KAMS í¬ë¡¤ë§ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def crawl_seoul_notice():
    """ì„œìš¸íŠ¹ë³„ì‹œ í¬ë¡¤ë§ (ë””ìì¸ ë‰´ìŠ¤)"""
    list_url = "https://news.seoul.go.kr/culture/archives/category/design-news_c1/business_design_c1/news_design-news-n1"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print(f"--- Seoul í¬ë¡¤ë§ ì‹œì‘: {list_url} ---")
    
    try:
        response = requests.get(list_url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì„œìš¸ì‹œ ì‚¬ì´íŠ¸ëŠ” archives/ìˆ«ì í˜•íƒœì˜ ë§í¬ë¥¼ ì§ì ‘ ì°¾ì•„ì•¼ í•¨
        all_links = soup.find_all('a', href=True)
        archive_items = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # archives/ìˆ«ì í˜•íƒœì˜ ë§í¬ ì°¾ê¸°
            if '/archives/' in href:
                # URLì—ì„œ ìˆ«ì ID ì¶”ì¶œ
                parts = href.split('/archives/')
                if len(parts) > 1:
                    id_part = parts[1].split('?')[0].split('/')[0]
                    if id_part.isdigit() and text and len(text) > 5:
                        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ë§Œ
                        archive_items.append({
                            'link': link,
                            'href': href,
                            'text': text
                        })
        
        if not archive_items:
            print("âŒ Seoul ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        results = []
        for item_data in archive_items:
            try:
                link_tag = item_data['link']
                href = item_data['href']
                title = item_data['text']
                
                if not title or not href:
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if href.startswith('/'):
                    final_link = urljoin('https://news.seoul.go.kr', href)
                elif href.startswith('http'):
                    final_link = href
                else:
                    final_link = urljoin(list_url, href)
                
                # ë‚ ì§œ ì¶”ì¶œ (ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°)
                date_text = 'ë‚ ì§œ ì—†ìŒ'
                parent = link_tag.parent
                if parent:
                    # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œ ì°¾ê¸°
                    date_elements = parent.select('.date, .post-date, time, [class*="date"], [datetime]')
                    if date_elements:
                        date_text = date_elements[0].get_text(strip=True)
                        if not date_text and date_elements[0].get('datetime'):
                            date_text = date_elements[0].get('datetime')
                    else:
                        # í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                        text = parent.get_text()
                        date_match = re.search(r'(\d{4}[.-]\d{2}[.-]\d{2})', text)
                        if date_match:
                            date_text = date_match.group(1)
                
                results.append({
                    'number': '',
                    'title': title,
                    'date': date_text,
                    'link': final_link
                })
            except Exception as e:
                print(f"  âš ï¸ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        if results:
            print(f"ì´ {len(results)}ê±´ì˜ Seoul ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            db = init_firebase()
            new_count = 0
            for item in results:
                if check_and_save(db, item, source='Seoul'):
                    new_count += 1
            print(f"\n=== Seoul ì‹¤í–‰ ì™„ë£Œ: {new_count}ê±´ ì‹ ê·œ ì €ì¥ ë° ì•Œë¦¼ ì „ì†¡ ===")
        else:
            print("\nSeoul ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"Seoul í¬ë¡¤ë§ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def crawl_seoul_public_art():
    """ì„œìš¸ ê³µê³µë¯¸ìˆ  ê³µëª¨ í¬ë¡¤ë§ (ë””ìì¸ ë‰´ìŠ¤ì—ì„œ ê³µê³µë¯¸ìˆ  ê³µëª¨ í•„í„°ë§)"""
    # ê³µê³µë¯¸ìˆ  ê³µëª¨ëŠ” ë””ìì¸ ë‰´ìŠ¤ í˜ì´ì§€ì— í¬í•¨ë˜ì–´ ìˆìŒ
    list_url = "https://news.seoul.go.kr/culture/archives/category/design-news_c1/business_design_c1/news_design-news-n1"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print(f"--- Seoul ê³µê³µë¯¸ìˆ  ê³µëª¨ í¬ë¡¤ë§ ì‹œì‘ (ë””ìì¸ ë‰´ìŠ¤ì—ì„œ í•„í„°ë§) ---")
    
    try:
        response = requests.get(list_url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ul li a[href*="archives"] ì„ íƒì ì‚¬ìš©
        archive_links = soup.select('ul li a[href*="archives"]')
        
        if not archive_links:
            print("âŒ Seoul ê³µê³µë¯¸ìˆ  ê³µëª¨ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        results = []
        for link_tag in archive_links:
            try:
                href = link_tag.get('href', '')
                title = link_tag.get_text(strip=True)
                
                if not title or not href or len(title) < 5:
                    continue
                
                # ê³µê³µë¯¸ìˆ  ê³µëª¨ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                public_art_keywords = ['ê³µê³µë¯¸ìˆ ', 'ë¯¸ìˆ ì‘í’ˆ', 'ì¡°í˜•ë¬¼', 'ê³µëª¨', 'ì„¤ì¹˜', 'ì‘ê°€']
                if not any(keyword in title for keyword in public_art_keywords):
                    continue
                
                # archives/ìˆ«ì í˜•íƒœì¸ì§€ í™•ì¸
                if '/archives/' not in href:
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if href.startswith('/'):
                    final_link = urljoin('https://news.seoul.go.kr', href)
                elif href.startswith('http'):
                    final_link = href
                else:
                    final_link = urljoin(list_url, href)
                
                # ë‚ ì§œ ì¶”ì¶œ (ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°)
                date_text = 'ë‚ ì§œ ì—†ìŒ'
                parent = link_tag.parent
                if parent:
                    # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œ ì°¾ê¸°
                    date_elements = parent.select('.date, .post-date, time, [class*="date"], [datetime]')
                    if date_elements:
                        date_text = date_elements[0].get_text(strip=True)
                        if not date_text and date_elements[0].get('datetime'):
                            date_text = date_elements[0].get('datetime')
                    else:
                        # í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                        text = parent.get_text()
                        date_match = re.search(r'(\d{4}[.-]\d{2}[.-]\d{2})', text)
                        if date_match:
                            date_text = date_match.group(1)
                
                results.append({
                    'number': '',
                    'title': title,
                    'date': date_text,
                    'link': final_link
                })
            except Exception as e:
                print(f"  âš ï¸ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        if results:
            print(f"ì´ {len(results)}ê±´ì˜ Seoul ê³µê³µë¯¸ìˆ  ê³µëª¨ ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            db = init_firebase()
            new_count = 0
            for item in results:
                if check_and_save(db, item, source='SeoulPublicArt'):
                    new_count += 1
            print(f"\n=== Seoul ê³µê³µë¯¸ìˆ  ê³µëª¨ ì‹¤í–‰ ì™„ë£Œ: {new_count}ê±´ ì‹ ê·œ ì €ì¥ ë° ì•Œë¦¼ ì „ì†¡ ===")
        else:
            print("\nSeoul ê³µê³µë¯¸ìˆ  ê³µëª¨ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"Seoul ê³µê³µë¯¸ìˆ  ê³µëª¨ í¬ë¡¤ë§ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
    crawl_lh_notice()
    crawl_kams_notice()
    crawl_seoul_notice()
    crawl_seoul_public_art()
